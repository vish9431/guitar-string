{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 2s 269ms/step - loss: 4.4240 - accuracy: 0.1842 - val_loss: 2.5875 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.2224 - accuracy: 0.1316 - val_loss: 2.2792 - val_accuracy: 0.3000\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 2.4474 - accuracy: 0.1316 - val_loss: 2.1299 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.9251 - accuracy: 0.3684 - val_loss: 2.2217 - val_accuracy: 0.2000\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.7776 - accuracy: 0.3947 - val_loss: 2.3882 - val_accuracy: 0.2000\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.8413 - accuracy: 0.3421 - val_loss: 2.4130 - val_accuracy: 0.2000\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 1.8280 - accuracy: 0.3684 - val_loss: 2.3648 - val_accuracy: 0.2000\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1.7342 - accuracy: 0.3684 - val_loss: 2.2770 - val_accuracy: 0.2000\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.6231 - accuracy: 0.3947 - val_loss: 2.1431 - val_accuracy: 0.3000\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.5078 - accuracy: 0.3947 - val_loss: 1.9974 - val_accuracy: 0.2000\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.4283 - accuracy: 0.4737 - val_loss: 1.8850 - val_accuracy: 0.2000\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.3803 - accuracy: 0.4474 - val_loss: 1.7982 - val_accuracy: 0.1000\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.3803 - accuracy: 0.5263 - val_loss: 1.7528 - val_accuracy: 0.2000\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.3864 - accuracy: 0.4474 - val_loss: 1.7392 - val_accuracy: 0.2000\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.3827 - accuracy: 0.3684 - val_loss: 1.7415 - val_accuracy: 0.2000\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.3528 - accuracy: 0.4474 - val_loss: 1.7455 - val_accuracy: 0.4000\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 386ms/step - loss: 1.3275 - accuracy: 0.5263 - val_loss: 1.7624 - val_accuracy: 0.4000\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 1.2826 - accuracy: 0.4737 - val_loss: 1.7845 - val_accuracy: 0.3000\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.2686 - accuracy: 0.4737 - val_loss: 1.7947 - val_accuracy: 0.3000\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.2555 - accuracy: 0.4737 - val_loss: 1.7847 - val_accuracy: 0.3000\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.2439 - accuracy: 0.5000 - val_loss: 1.7733 - val_accuracy: 0.3000\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.2313 - accuracy: 0.5000 - val_loss: 1.7431 - val_accuracy: 0.3000\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.2110 - accuracy: 0.5000 - val_loss: 1.7216 - val_accuracy: 0.3000\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 1.2117 - accuracy: 0.5000 - val_loss: 1.7255 - val_accuracy: 0.1000\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.2265 - accuracy: 0.5000 - val_loss: 1.7072 - val_accuracy: 0.1000\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 1.2232 - accuracy: 0.5000 - val_loss: 1.6462 - val_accuracy: 0.2000\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.1758 - accuracy: 0.5789 - val_loss: 1.6350 - val_accuracy: 0.3000\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.1479 - accuracy: 0.5263 - val_loss: 1.6549 - val_accuracy: 0.3000\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.1450 - accuracy: 0.5000 - val_loss: 1.6539 - val_accuracy: 0.3000\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1649 - accuracy: 0.5263 - val_loss: 1.6383 - val_accuracy: 0.4000\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.1788 - accuracy: 0.5789 - val_loss: 1.6020 - val_accuracy: 0.4000\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.1597 - accuracy: 0.5526 - val_loss: 1.5688 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1254 - accuracy: 0.5263 - val_loss: 1.5604 - val_accuracy: 0.3000\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.1015 - accuracy: 0.5789 - val_loss: 1.5531 - val_accuracy: 0.3000\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.1062 - accuracy: 0.5263 - val_loss: 1.5652 - val_accuracy: 0.2000\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 1.1318 - accuracy: 0.5263 - val_loss: 1.5587 - val_accuracy: 0.2000\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.1525 - accuracy: 0.5000 - val_loss: 1.5424 - val_accuracy: 0.3000\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.1496 - accuracy: 0.5263 - val_loss: 1.5494 - val_accuracy: 0.2000\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.1239 - accuracy: 0.5000 - val_loss: 1.5787 - val_accuracy: 0.2000\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 1.1052 - accuracy: 0.5000 - val_loss: 1.6174 - val_accuracy: 0.2000\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 1.1168 - accuracy: 0.5000 - val_loss: 1.6502 - val_accuracy: 0.3000\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.1324 - accuracy: 0.5000 - val_loss: 1.6201 - val_accuracy: 0.3000\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.1314 - accuracy: 0.4737 - val_loss: 1.5445 - val_accuracy: 0.3000\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 1.1086 - accuracy: 0.5263 - val_loss: 1.4687 - val_accuracy: 0.4000\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0791 - accuracy: 0.5526 - val_loss: 1.4375 - val_accuracy: 0.4000\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 1.0496 - accuracy: 0.5789 - val_loss: 1.4557 - val_accuracy: 0.4000\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.0496 - accuracy: 0.5789 - val_loss: 1.4981 - val_accuracy: 0.3000\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.0826 - accuracy: 0.5526 - val_loss: 1.5058 - val_accuracy: 0.3000\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 1.0991 - accuracy: 0.5526 - val_loss: 1.4651 - val_accuracy: 0.4000\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.1030 - accuracy: 0.5789 - val_loss: 1.4058 - val_accuracy: 0.3000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.4058 - accuracy: 0.3000\n",
      "Test accuracy: 30.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define constants\n",
    "DATA_DIR = 'GuitarNotes/train'  # Replace with your dataset path\n",
    "CLASS_NAMES = os.listdir(DATA_DIR)\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "SAMPLE_RATE = 44100  # Adjust based on your audio data\n",
    "DURATION = 2  # Duration of each audio clip in seconds\n",
    "NUM_MFCCS = 20  # Number of MFCCs to extract\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50  # Adjust based on your needs\n",
    "MAX_NUM_FRAMES = 173\n",
    "\n",
    "# Function to load and preprocess audio files\n",
    "def load_and_preprocess_data(data_dir):\n",
    "    X, y = [], []\n",
    "    for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for audio_file in os.listdir(class_dir):\n",
    "            audio_path = os.path.join(class_dir, audio_file)\n",
    "            audio, _ = librosa.load(audio_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=NUM_MFCCS)\n",
    "            # Transpose MFCCs to shape (num_frames, num_mfccs)\n",
    "            mfccs = mfccs.T  # Transpose the MFCCs\n",
    "            \n",
    "            if mfccs.shape[0] < MAX_NUM_FRAMES:\n",
    "                mfccs = np.pad(mfccs, ((0, MAX_NUM_FRAMES - mfccs.shape[0]), (0, 0)), 'constant')\n",
    "            elif mfccs.shape[0] > MAX_NUM_FRAMES:\n",
    "                mfccs = mfccs[:MAX_NUM_FRAMES, :]\n",
    "            \n",
    "            X.append(mfccs[:, :, np.newaxis])\n",
    "            y.append(class_idx)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and preprocess the data\n",
    "X, y = load_and_preprocess_data(DATA_DIR)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3])),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.GlobalAveragePooling2D(),  # Use Global Average Pooling\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the model if needed\n",
    "# model.save('audio_classification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 173, 25, 1)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process(audio_path):\n",
    "    audio, _ = librosa.load(audio_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=NUM_MFCCS)\n",
    "                # Transpose MFCCs to shape (num_frames, num_mfccs)\n",
    "    mfccs = mfccs.T  # Transpose the MFCCs\n",
    "                \n",
    "    if mfccs.shape[0] < MAX_NUM_FRAMES:\n",
    "        mfccs = np.pad(mfccs, ((0, MAX_NUM_FRAMES - mfccs.shape[0]), (0, 0)), 'constant')\n",
    "    elif mfccs.shape[0] > MAX_NUM_FRAMES:\n",
    "        mfccs = mfccs[:MAX_NUM_FRAMES, :]\n",
    "    \n",
    "    return np.array([mfccs[:, :, np.newaxis]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 820ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(process('k.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'D', 'Ehi', 'Elo', 'G']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def convert(file):  \n",
    "# convert mp3 to wav file\n",
    "    subprocess.call(['ffmpeg', '-i', file,   \n",
    "                 'r.wav'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('raw.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw.mp3', 'rb') as file:\n",
    "    with open('k.wav', 'wb') as f:\n",
    "        f.write(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(audio_path):\n",
    "    data = process(audio_path)\n",
    "    prediction = model.predict(data)\n",
    "    prediction = CLASS_NAMES[np.argmax(prediction)]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('raw.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abc.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    model,\n",
    "    input_signature=[tf.TensorSpec(shape=[None, 173, 20, 1], dtype=tf.float32, name='x')],\n",
    "    opset=13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(onnx_model, \"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = ort.InferenceSession('model.onnx', providers=['AzureExecutionProvider', 'CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.06233804, 0.11958687, 0.14967401, 0.19894259, 0.02399746,\n",
       "         0.445461  ]], dtype=float32)]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(None, {'x': process('raw.wav')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
